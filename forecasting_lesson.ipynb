{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting \n",
    "\n",
    "![extrapolating](https://imgs.xkcd.com/comics/extrapolating.png)\n",
    "\n",
    "In this lesson, we will practice forecasting using the following methods:  \n",
    "\n",
    "- Last Observed Value  \n",
    "- Simple Average  \n",
    "- Moving Average\n",
    "- Previous Cycle \n",
    "- Holt's Linear Trend\n",
    "- Holt's Seasonal Trend\n",
    " \n",
    "\n",
    "![time series overview of methods in this lesson](forecasting_overview.png)\n",
    "\n",
    "In this lesson we will:\n",
    "- Acquire and prepare store sales data stored in our Codeup MySQL server.\n",
    "- Forecast future values using each modeling approach.\n",
    "- Evaluate competing model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for presentation purposes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# wrangle\n",
    "from env import user, password, host\n",
    "import os\n",
    "\n",
    "# transform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualize \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# working with dates\n",
    "from datetime import datetime\n",
    "\n",
    "# modeling\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import Holt, ExponentialSmoothing\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle\n",
    "\n",
    "We will acquire the data from our MySQL server and cache a local .csv version of our data if it is not already present.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_url(database):\n",
    "    '''\n",
    "    Returns a formatted string using credentials stored in env.py that can be passed to a pd.read_sql() function\n",
    "    '''\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{database}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_store_data():\n",
    "    '''\n",
    "    Returns a dataframe of all store data in the tsa_item_demand database and saves a local copy as a csv file.\n",
    "    '''\n",
    "    if os.path.exists('tsa_store_data.csv'):\n",
    "        df = pd.read_csv('tsa_store_data.csv')\n",
    "    else:\n",
    "        query = '''\n",
    "                SELECT *\n",
    "                FROM items\n",
    "                JOIN sales USING(item_id)\n",
    "                JOIN stores USING(store_id)\n",
    "                '''\n",
    "\n",
    "        df = pd.read_sql(query, get_db_url('tsa_item_demand'))\n",
    "        df.to_csv('tsa_store_data.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_store_data():\n",
    "    '''\n",
    "    Checks for a local cache of tsa_store_data.csv and if not present will run the get_store_data() function which acquires data from Codeup's mysql server\n",
    "    '''\n",
    "    filename = 'tsa_store_data.csv'\n",
    "    if os.path.isfile(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "    else:\n",
    "        df = get_store_data()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle_store_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert the `sale_date` column to be a datetime type.\n",
    "2. Set index and sort values by date\n",
    "3. Rename `sale_amount` to `quantity` to make the two columns easier to understand what the data represents. \n",
    "4. Create a new field called `sales_total` that is the product of `quantity` and `item_price`.\n",
    "5. Confirm our approach using `.head()`, `.info()`, and `.describe()`.\n",
    "6. Resample to a daily period.\n",
    "7. Drop the leap year days\n",
    "\n",
    "### Preliminary Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['store_id',\n",
       " 'item_id',\n",
       " 'item_upc14',\n",
       " 'item_upc12',\n",
       " 'item_brand',\n",
       " 'item_name',\n",
       " 'item_price',\n",
       " 'sale_id',\n",
       " 'sale_date',\n",
       " 'sale_amount',\n",
       " 'store_address',\n",
       " 'store_zipcode',\n",
       " 'store_city',\n",
       " 'store_state']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_store_data(df):\n",
    "    '''\n",
    "    Prepares raw store data for analysis and time series modeling.\n",
    "    '''\n",
    "    # convert the slae date to a datetime\n",
    "    df.sale_date = pd.to_datetime (df['sale_date'])\n",
    "    \n",
    "    # make that value our index value\n",
    "    df.set_index('sale_date').sort_index()\n",
    "    \n",
    "    # rename and messy columns that could get confusing\n",
    "    df = df.rename(columns={'sale_amount':'quantity'})\n",
    "    \n",
    "    # engineer sales total by multipling quantity by item price\n",
    "    df['sales_total'] = df.quantity * df.item_price\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep_store_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_upc14</th>\n",
       "      <th>item_upc12</th>\n",
       "      <th>item_brand</th>\n",
       "      <th>item_name</th>\n",
       "      <th>item_price</th>\n",
       "      <th>sale_id</th>\n",
       "      <th>sale_date</th>\n",
       "      <th>quantity</th>\n",
       "      <th>store_address</th>\n",
       "      <th>store_zipcode</th>\n",
       "      <th>store_city</th>\n",
       "      <th>store_state</th>\n",
       "      <th>sales_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35200264013</td>\n",
       "      <td>35200264013</td>\n",
       "      <td>Riceland</td>\n",
       "      <td>Riceland American Jazmine Rice</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>13</td>\n",
       "      <td>12125 Alamo Ranch Pkwy</td>\n",
       "      <td>78253</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>TX</td>\n",
       "      <td>10.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35200264013</td>\n",
       "      <td>35200264013</td>\n",
       "      <td>Riceland</td>\n",
       "      <td>Riceland American Jazmine Rice</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>11</td>\n",
       "      <td>12125 Alamo Ranch Pkwy</td>\n",
       "      <td>78253</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>TX</td>\n",
       "      <td>9.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35200264013</td>\n",
       "      <td>35200264013</td>\n",
       "      <td>Riceland</td>\n",
       "      <td>Riceland American Jazmine Rice</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>14</td>\n",
       "      <td>12125 Alamo Ranch Pkwy</td>\n",
       "      <td>78253</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>TX</td>\n",
       "      <td>11.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35200264013</td>\n",
       "      <td>35200264013</td>\n",
       "      <td>Riceland</td>\n",
       "      <td>Riceland American Jazmine Rice</td>\n",
       "      <td>0.84</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>13</td>\n",
       "      <td>12125 Alamo Ranch Pkwy</td>\n",
       "      <td>78253</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>TX</td>\n",
       "      <td>10.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35200264013</td>\n",
       "      <td>35200264013</td>\n",
       "      <td>Riceland</td>\n",
       "      <td>Riceland American Jazmine Rice</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5</td>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>10</td>\n",
       "      <td>12125 Alamo Ranch Pkwy</td>\n",
       "      <td>78253</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>TX</td>\n",
       "      <td>8.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_id  item_id   item_upc14   item_upc12 item_brand  \\\n",
       "0         1        1  35200264013  35200264013   Riceland   \n",
       "1         1        1  35200264013  35200264013   Riceland   \n",
       "2         1        1  35200264013  35200264013   Riceland   \n",
       "3         1        1  35200264013  35200264013   Riceland   \n",
       "4         1        1  35200264013  35200264013   Riceland   \n",
       "\n",
       "                        item_name  item_price  sale_id  sale_date  quantity  \\\n",
       "0  Riceland American Jazmine Rice        0.84        1 2013-01-01        13   \n",
       "1  Riceland American Jazmine Rice        0.84        2 2013-01-02        11   \n",
       "2  Riceland American Jazmine Rice        0.84        3 2013-01-03        14   \n",
       "3  Riceland American Jazmine Rice        0.84        4 2013-01-04        13   \n",
       "4  Riceland American Jazmine Rice        0.84        5 2013-01-05        10   \n",
       "\n",
       "            store_address  store_zipcode   store_city store_state  sales_total  \n",
       "0  12125 Alamo Ranch Pkwy          78253  San Antonio          TX        10.92  \n",
       "1  12125 Alamo Ranch Pkwy          78253  San Antonio          TX         9.24  \n",
       "2  12125 Alamo Ranch Pkwy          78253  San Antonio          TX        11.76  \n",
       "3  12125 Alamo Ranch Pkwy          78253  San Antonio          TX        10.92  \n",
       "4  12125 Alamo Ranch Pkwy          78253  San Antonio          TX         8.40  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 913000 entries, 0 to 912999\n",
      "Data columns (total 15 columns):\n",
      " #   Column         Non-Null Count   Dtype         \n",
      "---  ------         --------------   -----         \n",
      " 0   store_id       913000 non-null  int64         \n",
      " 1   item_id        913000 non-null  int64         \n",
      " 2   item_upc14     913000 non-null  int64         \n",
      " 3   item_upc12     913000 non-null  int64         \n",
      " 4   item_brand     913000 non-null  object        \n",
      " 5   item_name      913000 non-null  object        \n",
      " 6   item_price     913000 non-null  float64       \n",
      " 7   sale_id        913000 non-null  int64         \n",
      " 8   sale_date      913000 non-null  datetime64[ns]\n",
      " 9   quantity       913000 non-null  int64         \n",
      " 10  store_address  913000 non-null  object        \n",
      " 11  store_zipcode  913000 non-null  int64         \n",
      " 12  store_city     913000 non-null  object        \n",
      " 13  store_state    913000 non-null  object        \n",
      " 14  sales_total    913000 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(2), int64(7), object(5)\n",
      "memory usage: 104.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_upc14</th>\n",
       "      <th>item_upc12</th>\n",
       "      <th>item_price</th>\n",
       "      <th>sale_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>store_zipcode</th>\n",
       "      <th>sales_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>913000.000000</td>\n",
       "      <td>913000.000000</td>\n",
       "      <td>9.130000e+05</td>\n",
       "      <td>9.130000e+05</td>\n",
       "      <td>913000.000000</td>\n",
       "      <td>913000.000000</td>\n",
       "      <td>913000.000000</td>\n",
       "      <td>913000.000000</td>\n",
       "      <td>913000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.500000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>2.384688e+11</td>\n",
       "      <td>2.384688e+11</td>\n",
       "      <td>5.160400</td>\n",
       "      <td>456500.500000</td>\n",
       "      <td>52.250287</td>\n",
       "      <td>78224.800000</td>\n",
       "      <td>280.898866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.872283</td>\n",
       "      <td>14.430878</td>\n",
       "      <td>2.978005e+11</td>\n",
       "      <td>2.978005e+11</td>\n",
       "      <td>2.737764</td>\n",
       "      <td>263560.542223</td>\n",
       "      <td>28.801144</td>\n",
       "      <td>16.479087</td>\n",
       "      <td>227.686575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.500005e+09</td>\n",
       "      <td>8.500005e+09</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78201.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>3.367410e+10</td>\n",
       "      <td>3.367410e+10</td>\n",
       "      <td>2.970000</td>\n",
       "      <td>228250.750000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>78217.000000</td>\n",
       "      <td>98.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.500000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>7.103651e+10</td>\n",
       "      <td>7.103651e+10</td>\n",
       "      <td>5.195000</td>\n",
       "      <td>456500.500000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>78220.500000</td>\n",
       "      <td>233.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>6.030842e+11</td>\n",
       "      <td>6.030842e+11</td>\n",
       "      <td>7.520000</td>\n",
       "      <td>684750.250000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>78232.000000</td>\n",
       "      <td>402.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>8.846237e+11</td>\n",
       "      <td>8.846237e+11</td>\n",
       "      <td>9.640000</td>\n",
       "      <td>913000.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>78253.000000</td>\n",
       "      <td>2104.410000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            store_id        item_id    item_upc14    item_upc12  \\\n",
       "count  913000.000000  913000.000000  9.130000e+05  9.130000e+05   \n",
       "mean        5.500000      25.500000  2.384688e+11  2.384688e+11   \n",
       "std         2.872283      14.430878  2.978005e+11  2.978005e+11   \n",
       "min         1.000000       1.000000  8.500005e+09  8.500005e+09   \n",
       "25%         3.000000      13.000000  3.367410e+10  3.367410e+10   \n",
       "50%         5.500000      25.500000  7.103651e+10  7.103651e+10   \n",
       "75%         8.000000      38.000000  6.030842e+11  6.030842e+11   \n",
       "max        10.000000      50.000000  8.846237e+11  8.846237e+11   \n",
       "\n",
       "          item_price        sale_id       quantity  store_zipcode  \\\n",
       "count  913000.000000  913000.000000  913000.000000  913000.000000   \n",
       "mean        5.160400  456500.500000      52.250287   78224.800000   \n",
       "std         2.737764  263560.542223      28.801144      16.479087   \n",
       "min         0.600000       1.000000       0.000000   78201.000000   \n",
       "25%         2.970000  228250.750000      30.000000   78217.000000   \n",
       "50%         5.195000  456500.500000      47.000000   78220.500000   \n",
       "75%         7.520000  684750.250000      70.000000   78232.000000   \n",
       "max         9.640000  913000.000000     231.000000   78253.000000   \n",
       "\n",
       "         sales_total  \n",
       "count  913000.000000  \n",
       "mean      280.898866  \n",
       "std       227.686575  \n",
       "min         0.000000  \n",
       "25%        98.010000  \n",
       "50%       233.280000  \n",
       "75%       402.780000  \n",
       "max      2104.410000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will resample to daily aggregating our values using sum. The original granularity is daily, but there are multiple records of the same days across multiple stores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'RangeIndex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# resample by day, sum\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_resampled \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43md\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msales_total\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Drop February 29th from our dataset (a leap year date)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df_resampled\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:10550\u001b[0m, in \u001b[0;36mDataFrame.resample\u001b[0;34m(self, rule, axis, closed, label, convention, kind, loffset, base, on, level, origin, offset)\u001b[0m\n\u001b[1;32m  10534\u001b[0m \u001b[38;5;129m@doc\u001b[39m(NDFrame\u001b[38;5;241m.\u001b[39mresample, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_shared_doc_kwargs)\n\u001b[1;32m  10535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresample\u001b[39m(\n\u001b[1;32m  10536\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10548\u001b[0m     offset: TimedeltaConvertibleTypes \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m  10549\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Resampler:\n\u001b[0;32m> 10550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  10551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10552\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclosed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10561\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10562\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:8313\u001b[0m, in \u001b[0;36mNDFrame.resample\u001b[0;34m(self, rule, axis, closed, label, convention, kind, loffset, base, on, level, origin, offset)\u001b[0m\n\u001b[1;32m   8310\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_resampler\n\u001b[1;32m   8312\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_resampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8314\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclosed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8318\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8325\u001b[0m \u001b[43m    \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8326\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8327\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/resample.py:1423\u001b[0m, in \u001b[0;36mget_resampler\u001b[0;34m(obj, kind, **kwds)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;124;03mCreate a TimeGrouper and return our resampler.\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m tg \u001b[38;5;241m=\u001b[39m TimeGrouper(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m-> 1423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_resampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/resample.py:1599\u001b[0m, in \u001b[0;36mTimeGrouper._get_resampler\u001b[0;34m(self, obj, kind)\u001b[0m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ax, TimedeltaIndex):\n\u001b[1;32m   1597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TimedeltaIndexResampler(obj, groupby\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m-> 1599\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly valid with DatetimeIndex, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimedeltaIndex or PeriodIndex, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got an instance of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(ax)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1603\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'RangeIndex'"
     ]
    }
   ],
   "source": [
    "# resample by day, sum\n",
    "df_resampled = df.resample('d')[['quantity', 'sales_total']].sum()\n",
    "# Drop February 29th from our dataset (a leap year date)\n",
    "df_resampled.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \n",
    "df_resampled = df[df_resampled.index !='2016-02=29']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resampled.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "\n",
    "1. We will use the training proportion method to split.    \n",
    "2. Identify the total length of the dataframe and multiple by `train_prop` to get the number of rows that equates to the first x% of the dataframe, which equates to the first x% of the time covered in the data.   (`x = train_prop * 100`)  \n",
    "3. Select row indices from 0 up to the index representing x-percentile for train, and from the index representing x-percentile through the end of the dataframe for test. In both of these, we will reset the index in order to return dataframes sorted by datetime.  \n",
    "4. Return train and test dataframes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 50% of our data for train (round to integer for a whole number)\n",
    "train_size = \n",
    "# take 30% for validation\n",
    "validate_size = int(len(df_resampled) * .3)\n",
    "# get the remainder or rows for test\n",
    "test_size = \n",
    "# get the delineation point between validate and test by summing train and validate\n",
    "validate_end_index = \n",
    "\n",
    "# split into train, validation, test\n",
    "train = \n",
    "validate = \n",
    "test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Our Data\n",
    "\n",
    "Let's plot our data first, viewing where the data is split into train and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the values of quantity and sales over time by data subset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Helpful Evaluation Functions\n",
    "\n",
    "Before we try out different methods for forecasting sales and number of items sold, let's create a couple of functions that will be helpful in evaluating each of the methods that follow. \n",
    "\n",
    "`evaluate()` will compute the Mean Squared Error and the Rood Mean Squared Error to evaluate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function to compute rmse\n",
    "# calculate an error metric on the validation's target versus predictions\n",
    "def evaluate(target_var):\n",
    "    '''\n",
    "    This function will take the actual values of the target_var from validate, \n",
    "    and the predicted values stored in yhat_df, \n",
    "    and compute the rmse, rounding to 0 decimal places. \n",
    "    it will return the rmse. \n",
    "    '''\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plot_and_eval()` will use the evaluate function and also plot train and test values with the predicted values in order to visualize our performance to make comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and evaluate \n",
    "def plot_and_eval(target_var):\n",
    "    '''\n",
    "    This function takes in the target var name (string), and returns a plot\n",
    "    of the values of train for that variable, validate, and the predicted values from yhat_df. \n",
    "    it will als lable the rmse. \n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are planning on evaluating a lot of models. Let's create an easy to read dataframe called `eval_df`. We will eventually add the metrics we calculate for each of our models to this dataframe for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the empty dataframe\n",
    "eval_df = pd.DataFrame(columns=['model_type', 'target_var', 'rmse'])\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we could manually append the performance results of each model to this dataframe, its better to create a function that will do it for us.\n",
    "\n",
    "`append_eval_df()` will append evaluation metrics for each model into our eval_df data frame object. This function is dependent on `eval_df` already existing, which we accomplished in our previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to store rmse for comparison purposes\n",
    "def append_eval_df(model_type, target_var):\n",
    "    '''\n",
    "    this function takes in as arguments the type of model run, and the name of the target variable. \n",
    "    It returns the eval_df with the rmse appended to it for that model and target_var. \n",
    "    '''\n",
    "    rmse = evaluate(target_var)\n",
    "    d = {'model_type': [model_type], 'target_var': [target_var], 'rmse': [rmse]}\n",
    "    return pd.concat([eval_df, pd.DataFrame(d)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast\n",
    "\n",
    "Forecasting is another word for predicting time series data. As a reminder, we will work with the following approaches:\n",
    "\n",
    "#### Baseline Models\n",
    "1. Last Observed Value\n",
    "2. Simple Average\n",
    "3. Moving Average\n",
    "\n",
    "#### Non-Baseline Models\n",
    "4. Previous Cycle\n",
    "5. Holt's Linear Trend\n",
    "6. Holt's Seasonal Trend\n",
    "\n",
    "### Last Observed Value\n",
    "\n",
    "The simplest method for forecasting is to predict all future values to be the last observed value.  \n",
    "\n",
    "\n",
    "**Make Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the last item of sales total and assign to variable\n",
    "# indexing: last thing to the end, then the first element\n",
    "last_sales = \n",
    "\n",
    "# take the last item of quantity and assign to variable\n",
    "last_quantity = \n",
    "\n",
    "# let's make the prediction dataframe that we referenced a little earlier\n",
    "yhat_df = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see, when peeking into yhat_df, that every predicted value is the same.  \n",
    "\n",
    "**Plot Actual vs. Predicted Values**\n",
    "\n",
    "Now, let's plot actual and predicted values using our plot_and_eval() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the plot and eval function we defined earlier for both targets:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate** \n",
    "\n",
    "Evaluate using MSE and RMSE, and add evaluation metrics to `eval_df`. We will add our metrics via our `append_eval_df()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Average\n",
    "\n",
    "Take the simple average of historical values and use that value to predict future values.   \n",
    "\n",
    "This is a good option for an initial baseline. Every future datapoint (those in 'test') will be assigned the same value, and that value will be the overall mean of the values in train. \n",
    "\n",
    "\n",
    "**Make Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute simple average of sales_total (from train data)\n",
    "avg_sales = \n",
    "\n",
    "# compute simple average of quantity (from train data)\n",
    "avg_quantity = \n",
    "\n",
    "\n",
    "yhat_df = pd.DataFrame({'sales_total': ,\n",
    "                        'quantity': },\n",
    "                        index=validate.index)\n",
    "\n",
    "yhat_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Actual vs. Predicted Values\n",
    "\n",
    "Similar to our handling of the previous baseline model, we can plot our `yhat_df` values against the actual values in validate. Our `plot_and_eval` function accomplishes this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    plot_and_eval(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate**\n",
    "\n",
    "Evaluate using MSE and RMSE, and add evaluation metrics to `eval_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    eval_df = append_eval_df(model_type='simple_average', \n",
    "                             target_var = col)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Average\n",
    "\n",
    "In this example, we will use a 30-day moving average to forecast. In other words, the average of the last period, here 30-days, will be used as the forecasted value. \n",
    "\n",
    "> Moving averages are moving and change over time, but only historically. Forecasts using the moving average use the average of the last period/window of time.\n",
    "\n",
    "![moving average forecast diagram](moving_average.png)\n",
    "\n",
    "**Make Predictions**\n",
    "\n",
    "There are several ways to obtain the mean of the last 30 periods in train. We will use the `.rolling()` method to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a 30 day rolling average, \n",
    "# use the most recent/last 30 day period value to predict forward. \n",
    "\n",
    "period = \n",
    "\n",
    "rolling_sales = \n",
    "rolling_quantity = \n",
    "\n",
    "# yhat_df = make_predictions()\n",
    "\n",
    "yhat_df = pd.DataFrame({'sales_total': rolling_sales,\n",
    "                        'quantity': rolling_quantity},\n",
    "                         index=validate.index)\n",
    "yhat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Actual vs. Predicted Values**\n",
    "\n",
    "Now, let's plot and evaluate the performance of our time series model using **Moving Average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    plot_and_eval(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate**\n",
    "\n",
    "Evaluate using MSE and RMSE, and add evaluation metrics to `eval_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    eval_df = append_eval_df(model_type='30d moving average', \n",
    "                             target_var = col)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out several other values for periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [4, 12, 26, 52, 104]\n",
    "\n",
    "for p in periods: \n",
    "    rolling_sales = r\n",
    "    rolling_quantity = \n",
    "    yhat_df = \n",
    "    model_type = \n",
    "    # lets update the eval_df (remember we have two targets!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is best so far? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the min rmse for each variable\n",
    "\n",
    "min_rmse_sales_total = \n",
    "min_rmse_quantity = \n",
    "\n",
    "# filter only the rows that match those rmse to find out \n",
    "# which models are best thus far\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as baselines are concerned, it looks like our 104 day moving average is a good starting point for comparisons.\n",
    "# Non-Baseline Models\n",
    "\n",
    "## Holt-Winters\n",
    "Two of the models that we will evaluate are based on Holt-Winters, which models on three elements:\n",
    "- A typical value (average)\n",
    "- A slope (trend) over time\n",
    "- And a cyclical repeating pattern (seasonality)\n",
    "\n",
    "Its worth looking at a seasonal decomposition plot of our target, to inspect these components:\n",
    "\n",
    "#### Seasonal Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    sm.tsa.seasonal_decompose(train[col].resample('W').mean()).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is both strong seasonality and a notable trend in both targets (`sales_total` and `quantity`). There are two Holt-Winters models that we will attempt\n",
    "- Holt's Linear Trend\n",
    "- Holt's Seasonal Model\n",
    "\n",
    "### Holt's Linear Trend\n",
    "\n",
    "Our approach will be similar to many other modeling processes we have performed:\n",
    "1. Create the object: `Holt()`\n",
    "2. Fit the object: `.fit()`\n",
    "3. Make predictions: `.predict()`\n",
    "\n",
    "The first set of hyperparameters are set when we call `Holt()`: \n",
    "\n",
    "- **exponential** = True/False (exponential vs. linear growth, additive vs. multiplicative)\n",
    "- **damped $\\phi$** = True/False\n",
    "    - with Holt, forecasts tend to increase or decrease indefinitely into the future.  To avoid absurd long term predictions, use the Damped method (True) which sets a damping parameter between 0< Ï• <1. \n",
    "\n",
    "A second set of hyperparameters are set when we call `.fit()`: \n",
    "\n",
    "- **smoothing_level ($\\alpha$)**: value between (0,1)\n",
    "    - Closer to 0, the level doesn't change with each new observation\n",
    "    - Closer to 1, the level reacts strongly with each new observation\n",
    "- **smoothing_slope ($\\beta$)**: value between (0,1)\n",
    "    - Closer to 0, trend is not changing over time\n",
    "    - Closer to 1, trend is changing significantly over time\n",
    "- **optimized**: use the auto-optimization that allow statsmodels to automatically find an optimized value for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Holt's Linear Trend\n",
    "\n",
    "![diagram of holts linear trent](holts.png)\n",
    "\n",
    "**Make Predictions**\n",
    "\n",
    "Now, like we would when using sklearn, we will create the Holt object, fit the model, and make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    model = \n",
    "    model = \n",
    "    yhat_values = \n",
    "    yhat_df[col] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Actual vs. Predicted Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    plot_and_eval(target_var = col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    eval_df = append_eval_df(model_type = 'holts_optimized', \n",
    "                            target_var = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holt's Seasonal Trend\n",
    "\n",
    "Holt's Seasonal Trend is started by using `ExponentialSmoothing()`\n",
    "\n",
    "The process is similar to our previous model:\n",
    "1. Create the object: `ExponentialSmoothing()`\n",
    "2. Fit the object: `.fit()`\n",
    "3. Make predictions: `.forecast()`\n",
    "\n",
    "This function has several hyperparameters:\n",
    "- **seasonal_periods**: The number of periods representing one cycle of seasonality. This is why performing a decomposition plot can be valuable, as this number needs to be entered manually.\n",
    "- **trend**: Whether the overall trend is additive (`trend='add'`) or multiplicative (`trend='mul'`)\n",
    "- **seasonal**: Whether the seasonality is additive (`seasonal='add'`) or multiplicative (`seasonal='mul'`)\n",
    "- **damped**: If we want the trend to reduce over the length of the forecast to avoid absurd long term predictions, we can set `damped=True`\n",
    "\n",
    "Given our smaller dataset, rather than choosing any one combination of hyperparameters, we can create multiple models to test different combinations:\n",
    "\n",
    "___\n",
    "<center>Quantity</center>\n",
    "\n",
    "|model_name|seasonal_periods|trend|seasonal|damped|\n",
    "|---|---|---|---|---|\n",
    "|hst_quantity_fit1|365|add|add|False|\n",
    "|hst_quantity_fit2|365|add|mul|False|\n",
    "|hst_quantity_fit3|365|add|add|True|\n",
    "|hst_quantity_fit4|365|add|mul|True|\n",
    "\n",
    "___\n",
    "<center>Sales Total</center>\n",
    "\n",
    "|model_name|seasonal_periods|trend|seasonal|damped|\n",
    "|---|---|---|---|---|\n",
    "|hst_sales_fit1|365|add|add|False|\n",
    "|hst_sales_fit2|365|add|mul|False|\n",
    "|hst_sales_fit3|365|add|add|True|\n",
    "|hst_sales_fit4|365|add|mul|True|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(itertools.product(['add','mul'],[True, False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's play with some hyperparameter combinations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare our scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model contains a SSE attribute that we can use to compare performance. We can derive RMSE from SSE, but for now, we can just use SSE to look at the relative performance of our Holt's Seasonal Trend models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_score= 'placeholder'\n",
    "for model in hsts['sales_total']:\n",
    "    score = (hsts['sales_total'][model].sse / len(train)) ** .5\n",
    "    current_model = {model: score}\n",
    "    if best_score == 'placeholder':\n",
    "        best_score = current_model\n",
    "    else:\n",
    "        if list(best_score.values())[0] > list(current_model.values())[0]:\n",
    "            best_score = current_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions\n",
    "\n",
    "The `.forecast()` method for Holt's Seasonal models requires the number of periods the model is going to provide a prediction for **after** the end of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsts['quantity']['hst_fit_1'].forecast(validate.shape[0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_df = pd.DataFrame({'sales_total': hsts['sales_total']['hst_fit_1'].forecast(validate.shape[0] + 1),\n",
    "                           'quantity': hsts['quantity']['hst_fit_1'].forecast(validate.shape[0] + 1)},\n",
    "                          index=validate.index)\n",
    "yhat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    plot_and_eval(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    eval_df = append_eval_df(model_type = 'holts_seasonal', \n",
    "                            target_var = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.sort_values(by='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    plot_and_eval(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    eval_df = append_eval_df(model_type = 'holts_seasonal', \n",
    "                            target_var = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.sort_values(by='rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best implementation of Holt's Seasonal Trend is significantly outperforming all other models made thus far.\n",
    "\n",
    "### Predict Based on Previous Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take all the 2016 data points, compute the daily delta, year-over-year, average that delta over all the days, and adding that average to the previous year's value on a day will give you the forecast for that day. \n",
    "\n",
    "If a primary cycle is weekly, then you may want to do this on a week-over-week cadence.\n",
    "\n",
    "![diagram of forecasting with previous cycle](previous_cycle.png)\n",
    "\n",
    "In the below example:  \n",
    "1. Compute the 365 average year over year differences from 2013 through 2015\n",
    "2. Add that average delta to the values during 2015. \n",
    "3. Set the index in your yhat dataframe to represent the dates those predictions are make for. \n",
    "\n",
    "Let's get started....\n",
    "\n",
    "**Re-split data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split it up by year\n",
    "train = \n",
    "validate = \n",
    "test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find year over year diff. from 2013-2014 and 2014-2015, take the mean, and add to each value in 2015.\n",
    "yhat_df = \n",
    "# set yhat_df to index of validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot and Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    plot_and_eval(target_var = col)\n",
    "    eval_df = append_eval_df(model_type = 'previous year', target_var = col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Which model did the best? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.sort_values(by='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_total_min_rmse = eval_df.groupby('target_var')['rmse'].min()[0]\n",
    "\n",
    "quantity_min_rmse = eval_df.groupby('target_var')['rmse'].min()[1]\n",
    "\n",
    "# find which model that is\n",
    "eval_df[((eval_df.rmse == sales_total_min_rmse) | \n",
    "         (eval_df.rmse == quantity_min_rmse))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    x = eval_df[eval_df.target_var == col]['model_type']\n",
    "    y = eval_df[eval_df.target_var == col]['rmse']\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=x, y=y)\n",
    "    plt.title(col)\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on Test\n",
    "\n",
    "Now that we have identified our one best model for each target variable, we can evaluate its peformance on our test data. As a reminder, `.forecast()` allows us to make predictions, but the method always starts after the end of the training data. To get to the test data, we will need to increase the number of periods being predicted to be equal to the combined periods of validate and test. \n",
    "\n",
    "We altered our train-validate-test split to perform the previous cycle approach. Let's reset to the original train-validate-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_resampled[:train_size]\n",
    "validate = df_resampled[train_size:validate_end_index]\n",
    "test = df_resampled[validate_end_index:]\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_df = pd.DataFrame({'sales_total': hsts['sales_total']['hst_fit_1'].forecast(validate.shape[0] + test.shape[0] + 1),\n",
    "                           'quantity': hsts['quantity']['hst_fit_1'].forecast(validate.shape[0] + test.shape[0] + 1)})\n",
    "yhat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original test set started on 2016-12-31, so we can slice out that portion from our `yhat_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_df = yhat_df['2016-12-31':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_plot(target_var):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(train[target_var], color='#377eb8', label='train')\n",
    "    plt.plot(validate[target_var], color='#ff7f00', label='validate')\n",
    "    plt.plot(test[target_var], color='#4daf4a',label='test')\n",
    "    plt.plot(yhat_df[target_var], color='#a65628', label='yhat')\n",
    "    plt.legend()\n",
    "    plt.title(target_var)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_sales_total = sqrt(mean_squared_error(test['sales_total'], \n",
    "                                       yhat_df['sales_total']))\n",
    "\n",
    "rmse_quantity = sqrt(mean_squared_error(test['quantity'], \n",
    "                                       yhat_df['quantity']))\n",
    "\n",
    "print('FINAL PERFORMANCE OF MODEL ON TEST DATA')\n",
    "print('rmse-sales total: ', rmse_sales_total)\n",
    "print('rmse-quantity: ', rmse_quantity)\n",
    "for col in train.columns:\n",
    "    final_plot(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RMSE of our final model did get noticably worse on the test data. Its possible that while the performance of the model starts high, it degrades the further out it projects. Let's take a look at what a projection into 2018 would look like.\n",
    "\n",
    "## Forcasting Into Future\n",
    "\n",
    "Predicting 2018 simply requires us to extend the value passed to `.forecast()` by an additional 365 periods and then slicing out what we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = pd.DataFrame({'sales_total': hsts['sales_total']['hst_fit_1'].forecast(validate.shape[0] + test.shape[0] + 1 + 365),\n",
    "                           'quantity': hsts['quantity']['hst_fit_1'].forecast(validate.shape[0] + test.shape[0] + 1 + 365)})\n",
    "# how does forecast look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_plot(target_var):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(train[target_var], color='#377eb8', label='Train')\n",
    "    plt.plot(validate[target_var], color='#ff7f00', label='Validate')\n",
    "    plt.plot(test[target_var], color='#4daf4a', label='Test')\n",
    "    plt.plot(yhat_df[target_var], color='#a65628', label='yhat')\n",
    "    plt.plot(forecast[target_var], color='#984ea3', label='Forecast')\n",
    "    plt.title(target_var)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    final_plot(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set was made of synthetic data with a clear and observable pattern. It allows us to observe the risk of long term performance degredation. While `Holt's Seasonal Trend` outperformed `Previous Cycle` on validate, `Previous Cycle` would have probably been the best model to use. `Holt's Seasonal Trend` is failing to demonstrate the higher maximum value in each subsequent cycle. \n",
    "\n",
    ">**However, I can be confident in this claim because I have atypical knowledge that this data is extremely predictable. Real data is rarely as reliable, and conclusions are rarely as clear as the example we have shown here.**\n",
    "\n",
    "Forecasting is an art as much as it is a science, as the environment that created the values of our historical data may be very different from the environment that creates future values. \n",
    "\n",
    ">**\"It's tough to make predictions, especially about the future.\" - Yogi Berra**\n",
    "\n",
    "## Further Resources\n",
    "- Modeling with [SARIMAX](https://ds.codeup.com/timeseries/sarimax/)\n",
    "- Modeling with [SVR](https://ds.codeup.com/timeseries/svr/)\n",
    "- The [Prophet model](https://facebook.github.io/prophet/docs/installation.html#installation-in-python) from Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "The end result of this exercise should be a Jupyter notebook named `model`.\n",
    "\n",
    "Using [saas.csv](https://ds.codeup.com/saas.csv) or log data from API usage\n",
    "\n",
    "1. Split data (train/test) and resample by any period, except daily, and aggregate using the sum. \n",
    "2. Forecast, plot and evaluate using each at least 4 of the methods we discussed:\n",
    "    - Last Observed Value\n",
    "    - Simple Average\n",
    "    - Moving Average\n",
    "    - Holt's Linear Trend \n",
    "    - Holt's Seasonal Trend\n",
    "    - Based on previous year/month/etc., this is up to you.\n",
    "\n",
    "Bonus: \n",
    "1. Using the store item demand data, create a forecast of `sales_total` and `quantity` for 2018 using the `Previous Cycle` approach.  .  \n",
    "2. Predict 2018 total **monthly** sales for a single store and/or item by creating a model using prophet.\n",
    "3. Return a dataframe with the month, store_id, y-hat, and the confidence intervals (y-hat lower, y-hat upper).\n",
    "4. Plot the 2018 monthly sales predictions."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
